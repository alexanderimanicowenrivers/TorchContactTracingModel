{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feasibility of controlling COVID-19 outbreaks by isolation of cases and contacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from math import floor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Feasibility of controlling COVID-19 outbreaks by isolation of cases and contacts\" - https://www.thelancet.com/journals/langlo/article/PIIS2214-109X(20)30074-7/fulltext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.distributions as tpd\n",
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write this in pytorch and test same outputs \n",
    "\n",
    "# literal adaption from:\n",
    "# http://stackoverflow.com/questions/4643285/how-to-generate-random-numbers-that-follow-skew-normal-distribution-in-matlab\n",
    "# original at:\n",
    "# http://www.ozgrid.com/forum/showthread.php?t=108175\n",
    "def rand_skew_norm(fAlpha, fLocation, fScale):\n",
    "    \"\"\"Sample a random skew normal variable with skew = alpha, loc=xi and scale=omega\"\"\"\n",
    "\n",
    "    sigma = fAlpha / np.sqrt(1.0 + fAlpha**2) \n",
    "\n",
    "    afRN = np.random.randn(2)\n",
    "    u0 = afRN[0]\n",
    "    v = afRN[1]\n",
    "    u1 = sigma*u0 + np.sqrt(1.0 -sigma**2) * v \n",
    "\n",
    "    if u0 >= 0:\n",
    "        return u1*fScale + fLocation \n",
    "    return (-u1)*fScale + fLocation \n",
    "\n",
    "def randn_skew(N, alpha=0.0, xi=0., omega=1.):\n",
    "    \"\"\"Sample N random skew normal variable with skew = alpha, loc=xi and scale=omega\"\"\"\n",
    "    return [rand_skew_norm(alpha, xi, omega) for x in range(N)]\n",
    "\n",
    "def inf_fn(num_samples=None, inc_samp = None, k = None):\n",
    "    \"\"\"\n",
    "    Infection function to sample exposure time, using skewed normal\n",
    "    TODO: Convert it into pytorch\n",
    "    \"\"\"\n",
    "    # xi is locatiom, omega is scale, alpha is slant\n",
    "    out = []\n",
    "    for n,inc_samp in zip(num_samples, inc_samp):\n",
    "        tmp = np.array(randn_skew(int(n), alpha=k, xi=inc_samp, omega=2.))\n",
    "        tmp = (tmp<1)*1 + (tmp>1)*tmp # if out less than one set to one\n",
    "        out.append(tmp)\n",
    "    return np.concatenate(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outbreak_setup(num_initial_cases = None,\n",
    "                   incfn = None,\n",
    "                   prop_asym = None,\n",
    "                   delayfn = None,\n",
    "                   k = None):\n",
    "    \"\"\"\n",
    "    Reimplementation of https://github.com/cmmid/ringbp/blob/master/R/outbreak_setup.R\n",
    "    \"\"\"\n",
    "    # Set up table of initial cases\n",
    "    inc_samples = incfn.sample_n(num_initial_cases)\n",
    "    \n",
    "    case_data = pd.DataFrame({'exposure':[], 'asym':[], 'infector':[], 'missed':[], 'onset':[], 'new_cases':[]})\n",
    "\n",
    "    case_data['exposure'] = torch.zeros(num_initial_cases)\n",
    "    case_data['asym'] = tpd.bernoulli.Bernoulli(prop_asym).sample_n(num_initial_cases)\n",
    "    case_data['caseid'] = torch.arange(0,num_initial_cases, 1)\n",
    "    case_data['infector'] = torch.zeros(num_initial_cases)\n",
    "    case_data['missed'] = torch.ones(num_initial_cases) # bool\n",
    "    case_data['onset'] = inc_samples\n",
    "    case_data['new_cases'] = torch.zeros(num_initial_cases) \n",
    "    # set isolation time for cluster to minimum time of onset of symptoms + draw from delay distribution\n",
    "    case_data['isolated_time'] = inc_samples + delayfn.sample_n(num_initial_cases)\n",
    "    case_data['isolated'] = torch.zeros(num_initial_cases)  # bool\n",
    "\n",
    "    # if asympomatc than isolated time set to infinity\n",
    "    case_data['isolated_time'] = np.maximum((case_data['asym']*np.inf).fillna(0), case_data['isolated_time'])\n",
    "\n",
    "\n",
    "    return case_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_max(t):\n",
    "    \"max between N tensor\"\n",
    "    unsqueezed = [t_.unsqueeze(-1) for t_ in t]\n",
    "    combined = torch.cat(unsqueezed, dim=-1)\n",
    "    return torch.max(combined, dim=-1).values\n",
    "\n",
    "def torch_min(t):\n",
    "    \"max between N tensor\"\n",
    "    unsqueezed = [t_.unsqueeze(-1) for t_ in t]\n",
    "    combined = torch.cat(unsqueezed, dim=-1)\n",
    "    return torch.min(combined, dim=-1).values\n",
    "\n",
    "\n",
    "\n",
    "def outbreak_step(case_data = None, disp_iso = None, \n",
    "                   disp_com = None, r0isolated = None, \n",
    "                   r0community = None, prop_asym = None, \n",
    "                   incfn = None, delayfn = None, prop_ascertain = None, \n",
    "                   k = None, quarantine = None):\n",
    "    \"\"\"\n",
    "    Reimplementation of R function https://github.com/cmmid/ringbp/blob/master/R/outbreak_step.R\n",
    "    \"\"\"\n",
    "    # For each case in case_data, draw new_cases from a negative binomial distribution\n",
    "    # with an R0 and dispersion dependent on if isolated=TRUE\n",
    "    n_cases = case_data.shape[0]\n",
    "    cases_if_not_isolated = (1-case_data[\"isolated\"])*tpd.negative_binomial.NegativeBinomial(*convert_params(r0community, disp_com)).sample_n(n_cases).numpy()\n",
    "    # Negative binomial didn't work with mean 0 and dispersion 1 so I just initalised tensor of zero\n",
    "    #     cases_if_isolated = case_data[\"isolated\"]*tpd.negative_binomial.NegativeBinomial(*convert_params(r0isolated, disp_iso)).sample_n(n_cases).numpy()\n",
    "    cases_if_isolated = case_data[\"isolated\"]*torch.zeros(n_cases).numpy()\n",
    "    case_data['new_cases'] = cases_if_isolated + cases_if_not_isolated\n",
    "    new_case_data = case_data[case_data['new_cases']>0]\n",
    "    total_new_cases = int(case_data['new_cases'].sum())\n",
    "    # If no new cases drawn, outbreak is over so return case_data\n",
    "    if (total_new_cases == 0):\n",
    "        print('No new cases')\n",
    "        case_data.isolated = 1.0\n",
    "        effective_r0 = 0 \n",
    "        cases_in_gen = 0\n",
    "        return {\"case_data\": case_data, \"effective_r0\" : effective_r0, \"cases_in_gen\": cases_in_gen}\n",
    "    \n",
    "    # Compile a data.table for all new cases, new_cases is the amount of people that each infector has infected    \n",
    "    inc_samples = incfn.sample_n(total_new_cases)\n",
    "    prob_samples = pd.DataFrame({'exposure':[], 'asym':[], 'infector':[], 'missed':[], 'onset':[], 'new_cases':[]})\n",
    "    # time when new cases were exposed, a draw from serial interval based on infector's onset\n",
    "    prob_samples['exposure'] = inf_fn(new_case_data.new_cases, new_case_data.onset, k)\n",
    "    # records the infector of each new person    \n",
    "    prob_samples['infector'] = np.array([id_ for i, (n,id_) in enumerate(zip(new_case_data.new_cases, new_case_data.caseid)) for _ in range(int(n))])\n",
    "    # records when infector was isolated\n",
    "    prob_samples['infector_iso_time'] = np.array([isolatedt for i, (n,isolatedt) in enumerate(zip(new_case_data.new_cases, new_case_data.isolated_time)) for _ in range(int(n))])\n",
    "    # records if infector asymptomatic\n",
    "    prob_samples['infector_asym'] = np.array([asym for i, (n,asym) in enumerate(zip(new_case_data.new_cases, new_case_data.asym)) for _ in range(int(n))])\n",
    "    # draws a sample to see if this person is asymptomatic    \n",
    "    prob_samples['asym'] = tpd.bernoulli.Bernoulli(prop_asym).sample_n(total_new_cases)\n",
    "    # draws a sample to see if this person is traced\n",
    "    prob_samples['missed'] = tpd.bernoulli.Bernoulli(1-prop_ascertain).sample_n(total_new_cases)\n",
    "    prob_samples['incubfn_sample'] = inc_samples\n",
    "    # set isolation time for cluster to minimum time of onset of symptoms + draw from delay distribution\n",
    "    prob_samples['new_cases'] = torch.zeros(total_new_cases) \n",
    "    prob_samples[\"isolated\"] = torch.zeros(total_new_cases)\n",
    "\n",
    "    # filter out new cases prevented by isolation\n",
    "    prob_samples = prob_samples[prob_samples.exposure < prob_samples.infector_iso_time]\n",
    "    total_new_cases_revised = prob_samples.shape[0]\n",
    "    \n",
    "    # If no new cases drawn due to isolation, outbreak is over so return case_data\n",
    "    if (total_new_cases_revised == 0):\n",
    "        print('No total_new_cases_revised')\n",
    "        case_data.isolated = 1.0\n",
    "        effective_r0 = 0 \n",
    "        cases_in_gen = 0\n",
    "        return {\"case_data\": case_data, \"effective_r0\" : effective_r0, \"cases_in_gen\": cases_in_gen}\n",
    "    \n",
    "    # onset of new case is exposure + incubation period sample\n",
    "    prob_samples['onset']= prob_samples.exposure + prob_samples.incubfn_sample\n",
    "    \n",
    "    # cases whose parents are asymptomatic are automatically missed, or missed from random process\n",
    "    prob_samples['missed']= np.maximum((prob_samples.infector_asym>=1.)*1, prob_samples['missed'])\n",
    "\n",
    "    # If you are asymptomatic, your isolation time is Inf  \n",
    "    iso_time_asym = torch.tensor(((prob_samples['asym']*np.inf).fillna(0)).to_numpy())\n",
    "\n",
    "    # If you are not asymptomatic, but you are missed,\n",
    "    # you are isolated at your symptom onset\n",
    "    iso_time_missed = torch.tensor(((1-prob_samples['asym'])*\n",
    "                                    prob_samples['missed']).to_numpy())*(\n",
    "                                    torch.tensor(prob_samples['onset'].to_numpy())\n",
    "                                     + delayfn.sample_n(total_new_cases_revised))\n",
    "                                  \n",
    "\n",
    "    # If you are not asymptomatic and you are traced,\n",
    "    # you are isolated at max(onset,infector isolation time) # max(onset,infector_iso_time)\n",
    "    # Not sure this is the bet logic for this\n",
    "    ons_iso_max = torch_max([torch.tensor(prob_samples['infector_iso_time'].to_numpy()), torch.tensor(prob_samples['onset'].to_numpy())+ delayfn.sample_n(total_new_cases_revised)])\n",
    "    nasym_traced = torch.tensor((1-prob_samples['asym']).to_numpy())*torch.tensor((1-prob_samples['missed']).to_numpy())\n",
    "    # 0 * inf gives nan, so set back to zero\n",
    "    iso_time_traced = (ons_iso_max*nasym_traced)*(1-quarantine) + quarantine*torch.tensor(prob_samples['infector_iso_time'].to_numpy())\n",
    "    iso_time_traced[torch.isnan(iso_time_traced)] = 0\n",
    "    \n",
    "    # take max isolation time \n",
    "    prob_samples['isolated_time'] = torch_max([iso_time_asym.float(),iso_time_missed.float(),iso_time_traced.float() ])\n",
    "\n",
    "    # Chop out unneeded sample columns\n",
    "    prob_samples = prob_samples.drop(columns=['incubfn_sample', 'infector_iso_time', 'infector_asym'])\n",
    "    \n",
    "    # Set new case ids for new people\n",
    "    prob_samples['caseid'] = torch.arange(n_cases, n_cases+total_new_cases_revised, 1)\n",
    "\n",
    "    ## Number of new cases\n",
    "    cases_in_gen = prob_samples.shape[0]\n",
    "    \n",
    "    ## Estimate the effective r0\n",
    "    n_new_cases_non_isolated = prob_samples[prob_samples.isolated!=1].shape[0]\n",
    "    effective_r0 = cases_in_gen / n_new_cases_non_isolated  if n_new_cases_non_isolated>0 else 0 \n",
    "\n",
    "    # Everyone in case_data so far has had their chance to infect and are therefore considered isolated\n",
    "    case_data.isolated = 1.0\n",
    "\n",
    "    # bind original cases + new secondary cases\n",
    "    case_data = pd.concat([case_data, prob_samples])\n",
    "    \n",
    "    # neaten up indexes in table\n",
    "    case_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return {\"case_data\": case_data, \"effective_r0\" : effective_r0, \"cases_in_gen\": cases_in_gen}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outbreak_model(num_initial_cases = None, prop_ascertain = None,\n",
    "                           cap_max_days = None, cap_cases = None,\n",
    "                           r0isolated = None, r0community = None,\n",
    "                           disp_iso = None, disp_com = None,\n",
    "                           k = None, delay_shape = None,\n",
    "                           delay_scale = None, prop_asym = None,\n",
    "                           quarantine = None):\n",
    "    \"\"\"\n",
    "    Reimplementation of https://github.com/cmmid/ringbp/blob/master/R/outbreak_model.R\n",
    "    \"\"\"\n",
    "    # Incubation period sampling function\n",
    "    incfn = tpd.weibull.Weibull(concentration = 2.322737,\n",
    "                          scale = 6.492272)\n",
    "    # Onset to isolation delay sampling function\n",
    "    delayfn = tpd.weibull.Weibull(concentration = delay_shape, # 1.651524 \n",
    "                            scale = delay_scale) # 4.287786 \n",
    "    # Set initial values for loop indices\n",
    "    total_cases= num_initial_cases # 5 \n",
    "    latest_onset = 0\n",
    "    extinct = False\n",
    "    \n",
    "    # Initial setup\n",
    "    case_data = outbreak_setup(num_initial_cases = total_cases,\n",
    "                               incfn = incfn,\n",
    "                               prop_asym = prop_asym,\n",
    "                               delayfn = delayfn,\n",
    "                               k = k)\n",
    "    \n",
    "    # Preallocate\n",
    "    effective_r0_vect = []\n",
    "    cases_in_gen_vect = [ ]\n",
    "    \n",
    "#     pbar = tqdm(total = cap_max_days+1)\n",
    "    # Model loop\n",
    "    while (latest_onset < cap_max_days) and (total_cases < cap_cases) and (not extinct):\n",
    "        out = outbreak_step(case_data = case_data,\n",
    "                             disp_iso = disp_iso,\n",
    "                             disp_com = disp_com,\n",
    "                             r0isolated = r0isolated,\n",
    "                             r0community = r0community,\n",
    "                             incfn = incfn,\n",
    "                             delayfn = delayfn,\n",
    "                             prop_ascertain = prop_ascertain,\n",
    "                             k = k,\n",
    "                             quarantine = quarantine,\n",
    "                             prop_asym = prop_asym)\n",
    "        \n",
    "        case_data = out['case_data']\n",
    "        effective_r0_vect.append(out['effective_r0'])\n",
    "        cases_in_gen_vect.append(out['cases_in_gen'])\n",
    "        total_cases = case_data.shape[0]\n",
    "        latest_onset = np.max(case_data.onset)\n",
    "        extinct = torch.all(torch.tensor(case_data.isolated.to_numpy()).bool())\n",
    "#         pbar.update(1)\n",
    "        \n",
    "#     pbar.close()\n",
    "    if (latest_onset > cap_max_days):\n",
    "        print('Terminating from max days')\n",
    "    elif (total_cases > cap_cases):\n",
    "        print('Total cases surpassed cap')\n",
    "    else: \n",
    "        print('Terminating due to stochastic extinction')\n",
    "\n",
    "    # Prepare output, group into weeks\n",
    "    case_data['week'] = np.floor((case_data['onset']/7).to_numpy())\n",
    "    weekly_cases = case_data[['week', 'onset']].groupby(['week']).count()\n",
    "    weekly_cases.reset_index(drop=False, inplace=True)\n",
    "\n",
    "\n",
    "    # maximum outbreak week\n",
    "    max_week = floor(cap_max_days / 7)\n",
    "    \n",
    "    # add in missing weeks\n",
    "    new_df = pd.DataFrame({'week': torch.arange(0,max_week,), 'onset': torch.zeros(max_week)})\n",
    "    weekly_cases = pd.merge(new_df, weekly_cases, on='week', how='left').fillna(0)\n",
    "    weekly_cases = weekly_cases[['week', 'onset_y']]\n",
    "    weekly_cases['cumulative'] = np.cumsum(weekly_cases['onset_y'].to_numpy())\n",
    "    # Add effective R0\n",
    "    weekly_cases['effective_r0'] = np.array(effective_r0_vect).mean()\n",
    "    return weekly_cases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_params(mu, alpha):\n",
    "    \"\"\" \n",
    "    Convert mean/dispersion parameterization of a negative binomial to the ones scipy supports\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    mu : float \n",
    "       Mean of NB distribution.\n",
    "    alpha : float\n",
    "       Overdispersion parameter used for variance calculation.\n",
    "\n",
    "    See https://en.wikipedia.org/wiki/Negative_binomial_distribution#Alternative_formulations\n",
    "    \"\"\"\n",
    "    var = mu + alpha * mu ** 2\n",
    "    p = (var - mu) / var\n",
    "    r = mu ** 2 / (var - mu)\n",
    "    return r, p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put parameters that are grouped by disease into here\n",
    "delay = \"Wuhan\" #(\"SARS\", \"Wuhan\")\n",
    "delay_shape = 1.651524 #(1.651524, 2.305172)\n",
    "delay_scale = 4.287786 #(4.287786, 9.483875)\n",
    "\n",
    "# TODO: not super familiar with the original R notation below\n",
    "# k_group = list(tibble::tibble(\n",
    "#     theta = c(\"<1%\", \"15%\", \"30%\"),\n",
    "#     k = c(30, 1.95, 0.7)\n",
    "#   )),\n",
    "\n",
    "k =  1.95 #(30, 1.95, 0.7) # k\n",
    "\n",
    "\n",
    "index_R0 =  2.5 #(1.5, 2.5, 3.5)\n",
    "prop_asym = 0.1 #(0, 0.1)\n",
    "control_effectiveness = 0.4 #torch.arange(0, 1.2, 0.2)\n",
    "num_initial_cases = 40 #(5, 20, 40)\n",
    "\n",
    "# Fixed params\n",
    "cap_max_days = 365\n",
    "cap_cases = 10000 # notice they have capped the number of cases to a very small value, can increase this using sparse matrices\n",
    "r0isolated = 0\n",
    "disp_iso = 1\n",
    "disp_com = 0.16\n",
    "quarantine = 0 #bool\n",
    "\n",
    "# Samples per experiment \n",
    "n_samples = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexandercowen-rivers/miniconda3/envs/epidemic/lib/python3.7/site-packages/torch/distributions/distribution.py:134: UserWarning: sample_n will be deprecated. Use .sample((n,)) instead\n",
      "  warnings.warn('sample_n will be deprecated. Use .sample((n,)) instead', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total cases surpassed cap\n",
      "Total cases surpassed cap\n",
      "Total cases surpassed cap\n",
      "Total cases surpassed cap\n",
      "Total cases surpassed cap\n",
      "Total cases surpassed cap\n",
      "Total cases surpassed cap\n",
      "Total cases surpassed cap\n",
      "Total cases surpassed cap\n",
      "Total cases surpassed cap\n"
     ]
    }
   ],
   "source": [
    "case_data_list = [outbreak_model(num_initial_cases = num_initial_cases, prop_ascertain = control_effectiveness,\n",
    "               cap_max_days = cap_max_days, cap_cases = cap_cases,\n",
    "               r0isolated = r0isolated, r0community = index_R0,\n",
    "               disp_iso = disp_iso, disp_com = disp_com,\n",
    "               k = k, delay_shape = delay_shape,\n",
    "               delay_scale = delay_scale, prop_asym = prop_asym,\n",
    "               quarantine = quarantine) for _ in range(n_samples)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_list = list(map(lambda p: p['cumulative'].to_numpy(), case_data_list))\n",
    "onset_y_list = list(map(lambda p: p['onset_y'].to_numpy(), case_data_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_list = np.vstack(cumulative_list)\n",
    "onset_y_list = np.vstack(onset_y_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_list = pd.DataFrame(cumulative_list).melt()\n",
    "onset_y_list = pd.DataFrame(onset_y_list).melt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.lineplot(data=cumulative_list, x='variable', y='value', label='Cumulative Weekly Cases')\n",
    "sns.lineplot(data=onset_y_list, x='variable', y='value', label='Weekly New Cases')\n",
    "plt.xlabel('Week')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Weekly Cases')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
